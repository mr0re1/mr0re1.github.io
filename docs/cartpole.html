<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üóø</text></svg>">
    <title>orlov blog</title>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-H2T5FV9H0R"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'G-H2T5FV9H0R');
    </script>

    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-4bw+/aepP/YC94hEpVNVgiZdgIC5+VKNBQNGCHeKRQN+PtmoHDEXuppvnDJzQIu9" crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.1/dist/js/bootstrap.min.js" integrity="sha384-Rx+T1VzGupg4BHQYs2gCW9It+akI2MM/mndMCy36UVfodzcJcF0GGLxZIzObiEfa" crossorigin="anonymous"></script>

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link href="static/code.css" rel="stylesheet">
    
</head>


<body>
    <div class="container">
         <nav class="navbar navbar-expand-lg bg-body-tertiary">
    <div class="container-fluid">
        <a class="navbar-brand" href="/">üóø orlov blog</a>
    </div>
</nav>
        <h1>RL: solving cartpole</h1>
        <div>
            <span>Posted on 2018-06-18</span>
            <span>|</span>
            <span>
                <a href="https://github.com/mr0re1/mr0re1.github.io/blob/master/posts/cartpole_exercise.ipynb">source code</a>
            </span>
        </div>
        <hr>
        <div>
        <h1 id="cartpole-exercise">Cartpole: exercise</h1>
<p>This is my attempt to solve <a
href="https://openai.com/requests-for-research/#cartpole">OpenAI.
Requests for Research. Cartpole: for newcomers to RL</a>.</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> gym</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tqdm <span class="im">import</span> tqdm</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> warnings</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>warnings.filterwarnings(<span class="st">&#39;ignore&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>env <span class="op">=</span> gym.make(<span class="st">&#39;CartPole-v0&#39;</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>SEED <span class="op">=</span> <span class="dv">42</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>env.seed(SEED)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>np.random.seed(SEED)</span></code></pre></div>
<blockquote>
<p>Start with a simple linear model (that has only four parameters), and
use the sign of the weighted sum to choose between the two actions.</p>
</blockquote>
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> make_linear_model(weights):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> policy(state):</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>        weighted_sum <span class="op">=</span> np.dot(state, weights)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="dv">0</span> <span class="cf">if</span> weighted_sum <span class="op">&lt;</span> <span class="dv">0</span> <span class="cf">else</span> <span class="dv">1</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> policy</span></code></pre></div>
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> evaluate_policy(policy):</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Runs policy through environment, returns accumulated reward.&quot;&quot;&quot;</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    score, state, done <span class="op">=</span> <span class="dv">0</span>, env.reset(), <span class="va">False</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> <span class="kw">not</span> done:  <span class="co"># CartPole-v0 uses max_episode_steps=200</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>        action <span class="op">=</span> policy(state)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>        state, reward, done, _ <span class="op">=</span> env.step(action)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>        score <span class="op">+=</span> reward</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> score</span></code></pre></div>
<blockquote>
<p>The random guessing algorithm: generate 10,000 random configurations
of the model‚Äôs parameters, and pick the one that achieves the best
cumulative reward. It is important to choose the distribution over the
parameters correctly.</p>
</blockquote>
<div class="sourceCode" id="cb5"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate random weights (1000 is more than enough) with naive normal distribution.</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>random_weights <span class="op">=</span> np.random.standard_normal([<span class="dv">1000</span>, <span class="dv">4</span>])</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>evaluation_scores <span class="op">=</span> np.apply_along_axis(</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">lambda</span> weights: evaluate_policy(make_linear_model(weights)),</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    axis<span class="op">=</span><span class="dv">1</span>, <span class="co"># apply for each row</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    arr<span class="op">=</span>random_weights)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Best score:&quot;</span>, evaluation_scores.<span class="bu">max</span>(),</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>      <span class="st">&quot; best random weights: &quot;</span>, random_weights[evaluation_scores.argmax()])</span></code></pre></div>
<pre><code>Best score: 200.0  best random weights:  [-0.676922    0.61167629  1.03099952  0.93128012]</code></pre>
<p>Turns out one can ‚Äúsolve‚Äù cartpool with just a handful of random
weights.</p>
<blockquote>
<p>The hill-climbing algorithm: start with a random setting of the
parameters, add a small amount of noise to the parameters, and evaluate
the new parameter configuration. If it performs better than the old
configuration, discard the old configuration and accept the new one.
Repeat this process for some number of iterations. How long does it take
to achieve perfect performance?</p>
</blockquote>
<div class="sourceCode" id="cb7"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>climbing_iterations, noise_amplitude <span class="op">=</span> <span class="dv">100</span>, <span class="fl">0.5</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>weights <span class="op">=</span> np.random.standard_normal([<span class="dv">4</span>])</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>score <span class="op">=</span> evaluate_policy(make_linear_model(weights))</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>score_hist <span class="op">=</span> [score]</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> T <span class="kw">in</span> <span class="bu">range</span>(climbing_iterations):</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    noise <span class="op">=</span> np.random.uniform(low<span class="op">=-</span>noise_amplitude, high<span class="op">=</span>noise_amplitude, size<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>    n_weights <span class="op">=</span> weights <span class="op">+</span> noise</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>    n_score <span class="op">=</span> evaluate_policy(make_linear_model(n_weights))</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> n_score <span class="op">&gt;</span> score:</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>        weights,score <span class="op">=</span> n_weights, n_score</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>    score_hist.append(score)</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>plt.plot(score_hist)</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Best score:&quot;</span>, score, <span class="st">&quot; best weights: &quot;</span>, weights)</span></code></pre></div>
<pre><code>Best score: 200.0  best weights:  [-0.48271142  0.17168059  0.47158941  0.76685094]</code></pre>
<figure>
<img src="output_10_1.png" alt="png" />
<figcaption aria-hidden="true">png</figcaption>
</figure>
<blockquote>
<p>Policy gradient algorithm: here, instead of choosing the action as a
deterministic function of the sign of the weighted sum, make it so that
action is chosen randomly, but where the distribution over actions (of
which there are two) depends on the numerical output of the inner
product. Policy gradient prescribes a principled parameter update rule [
<a href="https://www.youtube.com/watch?v=oPGVsoBonLM">1</a>, <a
href="http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching_files/pg.pdf">2</a>].</p>
</blockquote>
<blockquote>
<p>Your goal is to implement this algorithm for the simple linear model,
and see how long it takes to converge.</p>
</blockquote>
<p>My solution is based on <a
href="http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching_files/pg.pdf">Lecture
7: Policy Gradient</a> from awesome <a
href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html">UCL
Course on RL</a> by David Silver.</p>
<h3 id="policy">Policy</h3>
<p>We‚Äôre trying to maximize objective function <span
class="math display">\[
J(\theta)=\mathbb{E}_{\pi_\theta}[Q^{\pi_\theta}(s, a)] =
\sum_{s \in \mathcal{S} }{d(s)}\sum_{a \in \mathcal{A} }{ \pi_\theta(s,
a) Q^{\pi_\theta}(s, a)}
\]</span> According to Policy Gradient Theorem <span
class="math display">\[
\nabla_\theta{J(\theta)} =
\mathbb{E}_{\pi_\theta}[\nabla_\theta\log\pi_\theta(s,
a)Q^{\pi_\theta}(s,a)]
\]</span></p>
<p>Let‚Äôs introduce abstract class <code>Policy</code> that represents
function <span class="math inline">\(\pi_\theta(s, a)\)</span>
parameterized by <span class="math inline">\(\theta\)</span>, where:</p>
<ul>
<li><code>Policy.parameters</code> is <span
class="math inline">\(\theta\)</span>;</li>
<li><code>Policy.probabilities(state)</code> returns vector <span
class="math inline">\(\langle \pi_\theta(s, a_1), \dots, \pi_\theta(s,
a_n) \rangle\)</span>;</li>
<li><code>Policy.sample(state)</code> samples action <span
class="math inline">\(a\)</span> for state <span
class="math inline">\(s\)</span>, according probabilities <span
class="math inline">\(\pi_\theta(s, a)\)</span>;</li>
<li><code>Policy.score(state, action)</code> is policy score function
defined as <span class="math inline">\(\nabla_\theta\log\pi_\theta(s,
a)\)</span>.</li>
</ul>
<div class="sourceCode" id="cb9"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Policy:</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, parameters):</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.parameters <span class="op">=</span> np.copy(parameters)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> probabilities(<span class="va">self</span>, state): <span class="cf">raise</span> <span class="va">NotImplemented</span>()</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> score(<span class="va">self</span>, state, action): <span class="cf">raise</span> <span class="va">NotImplemented</span>()</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> sample(<span class="va">self</span>, state):</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>        p <span class="op">=</span> <span class="va">self</span>.probabilities(state)</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.random.choice(<span class="bu">range</span>(<span class="bu">len</span>(p)), p<span class="op">=</span>p)</span></code></pre></div>
<h3 id="monte-carlo-policy-gradient">Monte-Carlo policy gradient</h3>
<p>Using <span class="math inline">\(G_t\)</span> as unbiased sample of
<span class="math inline">\(Q^{\pi_\theta}(s_t, a_t)\)</span>, and
learning rate <span class="math inline">\(\alpha\)</span></p>
<p><span class="math display">\[
\Delta\theta = \alpha \nabla_\theta\log\pi_\theta(s, a) G_t
\]</span></p>
<div class="sourceCode" id="cb10"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> run_monte_carlo_policy_gradient_ascent(policy, num_episodes, alpha, gamma):</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Takes a `policy` and performs `num_episodes` rounds of gradient ascent.</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Each round, run a full episode using `policy.sample` and collect history</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="co">    [(state, action, reward), ...].</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="co">    Compute value G using history and perform update to policy parameters.</span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>    episodes_reward <span class="op">=</span> np.zeros([num_episodes])</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> T <span class="kw">in</span> tqdm(<span class="bu">range</span>(num_episodes)):</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Run full episode to obtain history to run Monte-Carlo on.</span></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>        state, done, history <span class="op">=</span> env.reset(), <span class="va">False</span>, []</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">while</span> <span class="kw">not</span> done:</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>            action <span class="op">=</span> policy.sample(state)</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>            next_state, reward, done, _ <span class="op">=</span> env.step(action)</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>            history.append((state, action, reward))</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>            state <span class="op">=</span> next_state</span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>        G <span class="op">=</span> <span class="dv">0</span> <span class="co"># sampled value</span></span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> state, action, reward <span class="kw">in</span> <span class="bu">reversed</span>(history):</span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>            G <span class="op">=</span> reward <span class="op">+</span> gamma <span class="op">*</span> G</span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>            episodes_reward[T] <span class="op">+=</span> reward</span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Update policy parameters</span></span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>            policy.parameters <span class="op">+=</span> alpha <span class="op">*</span> policy.score(state, action) <span class="op">*</span> G</span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> episodes_reward</span></code></pre></div>
<div class="sourceCode" id="cb11"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_value_and_running_avg(v, window<span class="op">=</span><span class="dv">10</span>):</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> <span class="bu">list</span>(<span class="bu">range</span>(<span class="bu">len</span>(v)))</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    plt.plot(X, v, X, pd.rolling_mean(v, window))</span></code></pre></div>
<p>Now we need to implement a policy (<code>probability</code> and
<code>score</code> methods).</p>
<h3 id="softmax-policy">Softmax policy</h3>
<p>David‚Äôs lecture uses softmax as an example. Probabilities of actions
are computed as a softmax of weights, where weight is linear combination
of stat-action features and policy parameters.</p>
<p><span class="math display">\[
\pi_\theta(s, a_i) =
\frac {exp(\phi(s, a_i)^\top \theta)} {\sum_{a \in \mathcal{A}
}{exp(\phi(s, a)^\top \theta)}}
\]</span></p>
<p>The score function is</p>
<p><span class="math display">\[
\nabla_\theta\log\pi_\theta(s, a) = \phi(s, a) -
\mathbb{E}_{\pi_\theta}[\phi(s, \cdot)]
\]</span></p>
<h4 id="choosing-features-represenation-of-state-and-action.">Choosing
features represenation of state and action.</h4>
<h5 id="state-one-hot-encoded-action">State + one-hot encoded
action</h5>
<p>As an ML-noob my first thougt was ‚Äúif features convey all available
information it should be fine‚Äù.</p>
<p>So I‚Äôve decided to concatenate four variables of observations with <a
href="https://en.wikipedia.org/wiki/One-hot">one-hot</a> encoded action:
<span class="math display">\[\phi(s, a_i) = \langle s_1, s_2, s_3, s_4,
\delta_{1i}, \delta_{2i} \rangle\]</span> , where <span
class="math inline">\(\delta_{jk}\)</span> is <a
href="https://en.wikipedia.org/wiki/Kronecker_delta">Kronecker
delta</a>.</p>
<p>Turns out it was poor choise. Gradient ascent didn‚Äôt have an effect
on parameters associated with state (<span
class="math inline">\(\theta_1, \dots, \theta_4\)</span>), only action
specific parameters (<span class="math inline">\(\theta_5,
\theta_6\)</span>) were updated. The policy was ‚Äúblind‚Äù to the
environment. The reason of such behavior is pretty simple:</p>
<p><span class="math display">\[
\nabla_\theta\log\pi_\theta(s, a_i) =
\phi(s, a_i) - \mathbb{E}_{\pi_\theta}[\phi(s, \cdot)] = \\
\langle s_1, s_2, s_3, s_4, \delta_{1i}, \delta_{2i} \rangle -
\langle s_1, s_2, s_3, s_4, \pi_\theta(s, a_1), \pi_\theta(s, a_2)
\rangle = \\
\langle 0, 0, 0, 0, \delta_{1i} - \pi_\theta(s, a_1), \delta_{2i} -
\pi_\theta(s, a_2) \rangle
\]</span></p>
<h5 id="better-representation-state-action-crossing">Better
representation: State-Action Crossing</h5>
<p><span class="math display">\[
\phi(s, a_1) = \langle s_1, s_2, s_3, s_4, 0, 0, 0, 0 \rangle \\
\phi(s, a_2) = \langle 0, 0, 0, 0, s_1, s_2, s_3, s_4  \rangle
\]</span></p>
<div class="sourceCode" id="cb12"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> StateActionCrossingSoftmaxPolicy(Policy):</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> features(<span class="va">self</span>, state, action):</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> action <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> np.append(state, np.zeros([<span class="bu">len</span>(state)]))</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> np.append(np.zeros([<span class="bu">len</span>(state)]), state)</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> probabilities(<span class="va">self</span>, state):</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>        e_0 <span class="op">=</span> np.exp(np.dot(<span class="va">self</span>.features(state, <span class="dv">0</span>), <span class="va">self</span>.parameters))</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>        e_1 <span class="op">=</span> np.exp(np.dot(<span class="va">self</span>.features(state, <span class="dv">1</span>), <span class="va">self</span>.parameters))</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>        norm <span class="op">=</span> e_0 <span class="op">+</span> e_1</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> [ e_0 <span class="op">/</span> norm, e_1 <span class="op">/</span> norm ]</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> score(<span class="va">self</span>, state, action):</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>        p <span class="op">=</span> <span class="va">self</span>.probabilities(state)</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.features(state, action) <span class="op">-</span> <span class="bu">sum</span>([p[a] <span class="op">*</span> <span class="va">self</span>.features(state, a) <span class="cf">for</span> a <span class="kw">in</span> (<span class="dv">0</span>, <span class="dv">1</span>)])</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>plot_value_and_running_avg(</span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a>    run_monte_carlo_policy_gradient_ascent(</span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a>        StateActionCrossingSoftmaxPolicy(np.random.standard_normal([<span class="dv">8</span>])),</span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a>        num_episodes<span class="op">=</span><span class="dv">2000</span>, alpha<span class="op">=</span><span class="fl">0.001</span>, gamma<span class="op">=</span><span class="fl">.99</span>))</span></code></pre></div>
<pre><code>100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2000/2000 [00:45&lt;00:00, 43.59it/s]</code></pre>
<figure>
<img src="output_22_1.png" alt="png" />
<figcaption aria-hidden="true">png</figcaption>
</figure>
<p>This way to represent features is equivalent to having a parameters
matrix with number of rows equal to number of state features and a
column for each action.</p>
<p><span class="math display">\[
\pi_\theta(s, a_i) = \frac {e^{s^{\top}\theta_{:,i}}}
{\sum_j{e^{s^{\top}\theta_{:,j}}}}
\text{ where } \theta_{:,i} \text{ is i-th column of } \theta
\]</span> <span class="math display">\[
\nabla_\theta\log\pi_\theta(s, a_i) =
\nabla_\theta{s^{\top}\theta_{:,i}} - s \langle \pi_\theta(s, a_1),
\dots, \pi_\theta(s, a_n) \rangle^{\top}
\]</span></p>
<p>In other words score function looks like this:</p>
<p><span class="math display">\[
\nabla_\theta\log\pi_\theta(s, a_i)=
\begin{bmatrix}
-\pi_\theta(s, a_1)s_1 &amp; \dots  &amp; (1 - \pi_\theta(s, a_i))s_1
&amp; \dots &amp; -\pi_\theta(s, a_k)s_1 \\
-\pi_\theta(s, a_1)s_2 &amp; \dots  &amp; (1 - \pi_\theta(s, a_i))s_2
&amp; \dots &amp; -\pi_\theta(s, a_k)s_2 \\
\vdots                 &amp; \vdots &amp;
\vdots                      &amp; \vdots &amp; \vdots \\
-\pi_\theta(s, a_1)s_n &amp; \dots  &amp; (1 - \pi_\theta(s, a_i))s_n
&amp; \dots &amp; -\pi_\theta(s, a_k)s_n \\
\end{bmatrix}
\]</span></p>
<p>It makes code more compact:</p>
<div class="sourceCode" id="cb14"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MatrixSoftmaxPolicy(Policy):    </span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> probabilities(<span class="va">self</span>, state):</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>        e <span class="op">=</span> np.exp(np.matmul(state, <span class="va">self</span>.parameters))</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> e <span class="op">/</span> e.<span class="bu">sum</span>()</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> score(<span class="va">self</span>, state, action):</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>        s <span class="op">=</span> <span class="op">-</span>np.einsum(<span class="st">&#39;i,j-&gt;ij&#39;</span>, state, <span class="va">self</span>.probabilities(state))</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>        s[:,action] <span class="op">+=</span> state</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> s</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>plot_value_and_running_avg(</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>    run_monte_carlo_policy_gradient_ascent(</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>        MatrixSoftmaxPolicy(np.random.standard_normal([<span class="dv">4</span>, <span class="dv">2</span>])),</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>        num_episodes<span class="op">=</span><span class="dv">2000</span>, alpha<span class="op">=</span><span class="fl">0.001</span>, gamma<span class="op">=</span><span class="fl">.99</span>))</span></code></pre></div>
<pre><code>100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2000/2000 [00:39&lt;00:00, 50.74it/s]</code></pre>
<figure>
<img src="output_24_1.png" alt="png" />
<figcaption aria-hidden="true">png</figcaption>
</figure>
<h4 id="sigmoid-policy">Sigmoid policy</h4>
<p>Essentially Cartpole policy represents probability of two disjoint
outcomes: ‚Äúapply force -1, otherwise apply force +1‚Äù. Therefore it can
be represented as <span class="math inline">\(B(s)\)</span> where <span
class="math display">\[
\begin{cases}
\pi_\theta(s, 0)=B(s) \\
\pi_\theta(s, 1)=1-B(s)
\end{cases}
\]</span></p>
<p>Using softmax and trying to fit eight parameters appears to me as an
overcomplicated way to solve binominal regression. Let‚Äôs try to solve it
using sigmoid of linear combination of parameters and state only.</p>
<p><span class="math display">\[
\pi_\theta(s, a) =
\begin{cases}
\ \frac {e^{s^\top \theta}} {1 + e^{s^\top \theta}} \text{ if } a=0,  \\
\ 1 - \pi_\theta(s, 0) \text{ if } a=1.
\end{cases}
\]</span></p>
<p>It got prety nice score functuion:</p>
<p><span class="math display">\[
\nabla_\theta\log\pi_\theta(s,0) =
\nabla_\theta \log\frac {e^{s^\top \theta}} {1 + e^{s^\top\theta}} = \\
\nabla_\theta(s^\top\theta - \log(1 + e^{s^\top\theta})) =
s - \frac {\nabla_\theta(1 + e^{s^\top\theta})} {1 + e^{s^\top\theta}} =
\\
s - s \frac {e^{s^\top\theta}} {1 + e^{s^\top\theta}} =
s(1 - \pi_\theta(s, 0)) = s\pi_\theta(s, 1)
\]</span></p>
<p><span class="math display">\[
\nabla_\theta\log\pi_\theta(s,1) =
\nabla_\theta\log\frac {1} {1 + e^{s^\top\theta}} =
-\nabla_\theta\log(1 + e^{s^\top\theta})=-s\pi_\theta(s,0)
\]</span> Hope I got it right.</p>
<div class="sourceCode" id="cb16"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SigmoidPolicy(Policy):</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> probabilities(<span class="va">self</span>, state):</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>        z <span class="op">=</span> np.dot(<span class="va">self</span>.parameters, state)</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>        e_z <span class="op">=</span> np.exp(z)</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>        p_0 <span class="op">=</span> e_z <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> e_z)</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> [p_0, <span class="fl">1.0</span> <span class="op">-</span> p_0]</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> score(<span class="va">self</span>, state, action):</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>        p <span class="op">=</span> <span class="va">self</span>.probabilities(state)</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> action <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> p[<span class="dv">1</span>] <span class="op">*</span> state</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="op">-</span>p[<span class="dv">0</span>] <span class="op">*</span> state</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>plot_value_and_running_avg(</span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>    run_monte_carlo_policy_gradient_ascent(</span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>        SigmoidPolicy(np.random.standard_normal([<span class="dv">4</span>])),</span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>        num_episodes<span class="op">=</span><span class="dv">2000</span>, alpha<span class="op">=</span><span class="fl">0.001</span>, gamma<span class="op">=</span><span class="fl">.99</span>))</span></code></pre></div>
<pre><code>100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2000/2000 [00:26&lt;00:00, 76.30it/s]</code></pre>
<figure>
<img src="output_26_1.png" alt="png" />
<figcaption aria-hidden="true">png</figcaption>
</figure>
<p>It seems to work.</p>
<h3 id="non-linear-policy-with-tensorflow">Non-linear policy with
Tensorflow</h3>
<div class="sourceCode" id="cb18"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fully_connected(inp, ouptut_size, activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>):</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Auxiliary function. Creates a fully connected layer.&quot;&quot;&quot;</span></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>    inp_size <span class="op">=</span> inp.get_shape().as_list()[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>    w <span class="op">=</span> tf.Variable(tf.random_normal([inp_size, ouptut_size], stddev<span class="op">=</span><span class="fl">0.35</span>), name<span class="op">=</span><span class="st">&#39;W&#39;</span>)</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>    tf.summary.histogram(<span class="st">&#39;W&#39;</span>, w)</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>    b <span class="op">=</span> tf.Variable(tf.zeros([ouptut_size]), name<span class="op">=</span><span class="st">&#39;b&#39;</span>)</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>    tf.summary.histogram(<span class="st">&#39;b&#39;</span>, b)</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> tf.matmul(inp, w) <span class="op">+</span> b</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> activation <span class="op">==</span> <span class="st">&#39;relu&#39;</span>:</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> tf.nn.relu(res)</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> activation <span class="kw">is</span> <span class="va">None</span></span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> res</span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> choose_one(A, index, name<span class="op">=</span><span class="st">&quot;choose_one&quot;</span>):</span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Auxiliary function. Returns vector C, where C[i] = A[i, index[i]]&quot;&quot;&quot;</span></span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a>    rows,cols <span class="op">=</span> tf.shape(A)[<span class="dv">0</span>], tf.shape(A)[<span class="dv">1</span>] </span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a>    flat_indx <span class="op">=</span> tf.<span class="bu">range</span>(<span class="dv">0</span>, rows) <span class="op">*</span> cols <span class="op">+</span> index</span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tf.gather(tf.reshape(A, [<span class="op">-</span><span class="dv">1</span>]), flat_indx)</span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Actor:</span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_obs<span class="op">=</span><span class="va">None</span>, n_act<span class="op">=</span><span class="va">None</span>, n_hidden<span class="op">=</span><span class="va">None</span>, alpha<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb18-24"><a href="#cb18-24" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.observations <span class="op">=</span> tf.placeholder(tf.float32, shape<span class="op">=</span>[<span class="va">None</span>, n_obs])</span>
<span id="cb18-25"><a href="#cb18-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-26"><a href="#cb18-26" aria-hidden="true" tabindex="-1"></a>        fc1 <span class="op">=</span> fully_connected(<span class="va">self</span>.observations, n_hidden, activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>)</span>
<span id="cb18-27"><a href="#cb18-27" aria-hidden="true" tabindex="-1"></a>        fc2 <span class="op">=</span> fully_connected(fc1, n_act, activation<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb18-28"><a href="#cb18-28" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Probabilities of actions</span></span>
<span id="cb18-29"><a href="#cb18-29" aria-hidden="true" tabindex="-1"></a>        probabilities <span class="op">=</span> tf.nn.softmax(fc2)</span>
<span id="cb18-30"><a href="#cb18-30" aria-hidden="true" tabindex="-1"></a>        log_p <span class="op">=</span> tf.log(probabilities)</span>
<span id="cb18-31"><a href="#cb18-31" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pick one action</span></span>
<span id="cb18-32"><a href="#cb18-32" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.sample <span class="op">=</span> tf.squeeze(tf.multinomial(log_p, <span class="dv">1</span>), axis<span class="op">=</span>[<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb18-33"><a href="#cb18-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-34"><a href="#cb18-34" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Training</span></span>
<span id="cb18-35"><a href="#cb18-35" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.taken_actions <span class="op">=</span> tf.placeholder(tf.int32, shape<span class="op">=</span>[<span class="va">None</span>])</span>
<span id="cb18-36"><a href="#cb18-36" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.advantages <span class="op">=</span> tf.placeholder(tf.float32, shape<span class="op">=</span>[<span class="va">None</span>])</span>
<span id="cb18-37"><a href="#cb18-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-38"><a href="#cb18-38" aria-hidden="true" tabindex="-1"></a>        taken_p <span class="op">=</span> choose_one(log_p, <span class="va">self</span>.taken_actions)</span>
<span id="cb18-39"><a href="#cb18-39" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> <span class="op">-</span>tf.reduce_sum(taken_p <span class="op">*</span> <span class="va">self</span>.advantages, name<span class="op">=</span><span class="st">&quot;loss&quot;</span>)</span>
<span id="cb18-40"><a href="#cb18-40" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.train_step <span class="op">=</span> tf.train.RMSPropOptimizer(learning_rate<span class="op">=</span>alpha).minimize(loss)</span>
<span id="cb18-41"><a href="#cb18-41" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb18-42"><a href="#cb18-42" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> run_tf_policy_gradient(env, num_episodes, n_hidden<span class="op">=</span><span class="va">None</span>, alpha<span class="op">=</span><span class="va">None</span>, gamma<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb18-43"><a href="#cb18-43" aria-hidden="true" tabindex="-1"></a>    n_obs, n_act <span class="op">=</span> np.product(env.observation_space.shape), env.action_space.n</span>
<span id="cb18-44"><a href="#cb18-44" aria-hidden="true" tabindex="-1"></a>    episode_rewards <span class="op">=</span> np.zeros([num_episodes])</span>
<span id="cb18-45"><a href="#cb18-45" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-46"><a href="#cb18-46" aria-hidden="true" tabindex="-1"></a>    tf.reset_default_graph()</span>
<span id="cb18-47"><a href="#cb18-47" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-48"><a href="#cb18-48" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> tf.Session() <span class="im">as</span> sess:</span>
<span id="cb18-49"><a href="#cb18-49" aria-hidden="true" tabindex="-1"></a>        actor <span class="op">=</span> Actor(n_obs<span class="op">=</span>n_obs, n_act<span class="op">=</span>n_act, alpha<span class="op">=</span>alpha, n_hidden<span class="op">=</span>n_hidden)</span>
<span id="cb18-50"><a href="#cb18-50" aria-hidden="true" tabindex="-1"></a>        sess.run(tf.global_variables_initializer())</span>
<span id="cb18-51"><a href="#cb18-51" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb18-52"><a href="#cb18-52" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> T <span class="kw">in</span> tqdm(<span class="bu">range</span>(num_episodes)):</span>
<span id="cb18-53"><a href="#cb18-53" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Use Monte-Carlo method to sample state-action values.</span></span>
<span id="cb18-54"><a href="#cb18-54" aria-hidden="true" tabindex="-1"></a>            obs, done, history <span class="op">=</span> env.reset(), <span class="va">False</span>, []</span>
<span id="cb18-55"><a href="#cb18-55" aria-hidden="true" tabindex="-1"></a>            <span class="cf">while</span> <span class="kw">not</span> done:</span>
<span id="cb18-56"><a href="#cb18-56" aria-hidden="true" tabindex="-1"></a>                action <span class="op">=</span> sess.run(actor.sample, feed_dict<span class="op">=</span>{actor.observations: [obs]})[<span class="dv">0</span>]</span>
<span id="cb18-57"><a href="#cb18-57" aria-hidden="true" tabindex="-1"></a>                next_obs, reward, done, _ <span class="op">=</span> env.step(action)</span>
<span id="cb18-58"><a href="#cb18-58" aria-hidden="true" tabindex="-1"></a>                history.append((obs, action, reward))</span>
<span id="cb18-59"><a href="#cb18-59" aria-hidden="true" tabindex="-1"></a>                obs <span class="op">=</span> next_obs</span>
<span id="cb18-60"><a href="#cb18-60" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb18-61"><a href="#cb18-61" aria-hidden="true" tabindex="-1"></a>            observations <span class="op">=</span> np.zeros([<span class="bu">len</span>(history), n_obs])</span>
<span id="cb18-62"><a href="#cb18-62" aria-hidden="true" tabindex="-1"></a>            actions <span class="op">=</span> np.zeros([<span class="bu">len</span>(history)])</span>
<span id="cb18-63"><a href="#cb18-63" aria-hidden="true" tabindex="-1"></a>            values <span class="op">=</span> np.zeros([<span class="bu">len</span>(history)])</span>
<span id="cb18-64"><a href="#cb18-64" aria-hidden="true" tabindex="-1"></a>            G <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb18-65"><a href="#cb18-65" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> i, (obs, action, reward) <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">reversed</span>(history)):</span>
<span id="cb18-66"><a href="#cb18-66" aria-hidden="true" tabindex="-1"></a>                episode_rewards[T] <span class="op">+=</span> reward</span>
<span id="cb18-67"><a href="#cb18-67" aria-hidden="true" tabindex="-1"></a>                G <span class="op">=</span> G <span class="op">*</span> gamma <span class="op">+</span> reward</span>
<span id="cb18-68"><a href="#cb18-68" aria-hidden="true" tabindex="-1"></a>                observations[i, :], actions[i], values[i] <span class="op">=</span> obs, action, G</span>
<span id="cb18-69"><a href="#cb18-69" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb18-70"><a href="#cb18-70" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Fit policy</span></span>
<span id="cb18-71"><a href="#cb18-71" aria-hidden="true" tabindex="-1"></a>            sess.run(actor.train_step, feed_dict<span class="op">=</span>{</span>
<span id="cb18-72"><a href="#cb18-72" aria-hidden="true" tabindex="-1"></a>                actor.observations: observations,</span>
<span id="cb18-73"><a href="#cb18-73" aria-hidden="true" tabindex="-1"></a>                actor.taken_actions: actions,</span>
<span id="cb18-74"><a href="#cb18-74" aria-hidden="true" tabindex="-1"></a>                actor.advantages: values })</span>
<span id="cb18-75"><a href="#cb18-75" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb18-76"><a href="#cb18-76" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> episode_rewards</span>
<span id="cb18-77"><a href="#cb18-77" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb18-78"><a href="#cb18-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-79"><a href="#cb18-79" aria-hidden="true" tabindex="-1"></a>plot_value_and_running_avg(</span>
<span id="cb18-80"><a href="#cb18-80" aria-hidden="true" tabindex="-1"></a>    run_tf_policy_gradient(env, n_hidden<span class="op">=</span><span class="dv">32</span>, num_episodes<span class="op">=</span><span class="dv">1000</span>, alpha<span class="op">=</span><span class="fl">0.01</span>, gamma<span class="op">=</span><span class="fl">.99</span>))</span></code></pre></div>
<pre><code>100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:46&lt;00:00, 21.45it/s]</code></pre>
<figure>
<img src="output_29_1.png" alt="png" />
<figcaption aria-hidden="true">png</figcaption>
</figure>
        </div>

        <div class="giscus-wrap container">
            <script src="https://giscus.app/client.js" data-repo="mr0re1/mr0re1.github.io"
                data-repo-id="MDEwOlJlcG9zaXRvcnk0MjE0ODk2Mw==" data-category="Posts discussions"
                data-category-id="DIC_kwDOAoMkY84CYxFJ" data-mapping="pathname" data-strict="0"
                data-reactions-enabled="0" data-emit-metadata="0" data-input-position="bottom" data-theme="light"
                data-lang="en" crossorigin="anonymous" async>
                </script>
        </div>
    </div>
</body>

</html>